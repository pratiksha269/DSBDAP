{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824a9fc1",
   "metadata": {},
   "source": [
    "Snehal Joshi <br> 72030840C<br> 23211<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b4574",
   "metadata": {},
   "source": [
    "**Practical 7** <br> Part1: Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "037bdd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82df06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Harry is a special young wizard. Harry's parents, James and Lily Potter, were killed by Lord Voldemort when he was a \\\n",
    "        baby. He lives with and is raised by his Aunt Petunia, his mother's sister, and Uncle Vernon, along with their son \\\n",
    "        Dudley for the duration of the series.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994346e0",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8feb3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c196aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'is', 'a', 'special', 'young', 'wizard', '.', 'Harry', \"'s\", 'parents', ',', 'James', 'and', 'Lily', 'Potter', ',', 'were', 'killed', 'by', 'Lord', 'Voldemort', 'when', 'he', 'was', 'a', 'baby', '.', 'He', 'lives', 'with', 'and', 'is', 'raised', 'by', 'his', 'Aunt', 'Petunia', ',', 'his', 'mother', \"'s\", 'sister', ',', 'and', 'Uncle', 'Vernon', ',', 'along', 'with', 'their', 'son', 'Dudley', 'for', 'the', 'duration', 'of', 'the', 'series', '.']\n"
     ]
    }
   ],
   "source": [
    "word_token=word_tokenize(text)\n",
    "print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4d609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry is a special young wizard.', \"Harry's parents, James and Lily Potter, were killed by Lord Voldemort when he was a         baby.\", \"He lives with and is raised by his Aunt Petunia, his mother's sister, and Uncle Vernon, along with their son         Dudley for the duration of the series.\"]\n"
     ]
    }
   ],
   "source": [
    "sentence= sent_tokenize(text)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08542a",
   "metadata": {},
   "source": [
    "**POS Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63bf6172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Harry', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('special', 'JJ'), ('young', 'JJ'), ('wizard', 'NN'), ('.', '.'), ('Harry', 'NNP'), (\"'s\", 'POS'), ('parents', 'NNS'), (',', ','), ('James', 'NNP'), ('and', 'CC'), ('Lily', 'NNP'), ('Potter', 'NNP'), (',', ','), ('were', 'VBD'), ('killed', 'VBN'), ('by', 'IN'), ('Lord', 'NNP'), ('Voldemort', 'NNP'), ('when', 'WRB'), ('he', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('baby', 'NN'), ('.', '.'), ('He', 'PRP'), ('lives', 'VBZ'), ('with', 'IN'), ('and', 'CC'), ('is', 'VBZ'), ('raised', 'VBN'), ('by', 'IN'), ('his', 'PRP$'), ('Aunt', 'NNP'), ('Petunia', 'NNP'), (',', ','), ('his', 'PRP$'), ('mother', 'NN'), (\"'s\", 'POS'), ('sister', 'NN'), (',', ','), ('and', 'CC'), ('Uncle', 'NNP'), ('Vernon', 'NNP'), (',', ','), ('along', 'IN'), ('with', 'IN'), ('their', 'PRP$'), ('son', 'NN'), ('Dudley', 'NNP'), ('for', 'IN'), ('the', 'DT'), ('duration', 'NN'), ('of', 'IN'), ('the', 'DT'), ('series', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(word_token)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843d773",
   "metadata": {},
   "source": [
    "**Stopwords removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c98521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e3284b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'special', 'young', 'wizard', '.', 'Harry', \"'s\", 'parents', ',', 'James', 'Lily', 'Potter', ',', 'killed', 'Lord', 'Voldemort', 'baby', '.', 'He', 'lives', 'raised', 'Aunt', 'Petunia', ',', 'mother', \"'s\", 'sister', ',', 'Uncle', 'Vernon', ',', 'along', 'son', 'Dudley', 'duration', 'series', '.']\n"
     ]
    }
   ],
   "source": [
    "cleaned_words=[]\n",
    "for i in word_token:\n",
    "    if i not in stop_words:\n",
    "        cleaned_words.append(i)\n",
    "        \n",
    "print(cleaned_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f16fd",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e41804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23da7ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry  :  harri\n",
      "special  :  special\n",
      "young  :  young\n",
      "wizard  :  wizard\n",
      ".  :  .\n",
      "Harry  :  harri\n",
      "'s  :  's\n",
      "parents  :  parent\n",
      ",  :  ,\n",
      "James  :  jame\n",
      "Lily  :  lili\n",
      "Potter  :  potter\n",
      ",  :  ,\n",
      "killed  :  kill\n",
      "Lord  :  lord\n",
      "Voldemort  :  voldemort\n",
      "baby  :  babi\n",
      ".  :  .\n",
      "He  :  he\n",
      "lives  :  live\n",
      "raised  :  rais\n",
      "Aunt  :  aunt\n",
      "Petunia  :  petunia\n",
      ",  :  ,\n",
      "mother  :  mother\n",
      "'s  :  's\n",
      "sister  :  sister\n",
      ",  :  ,\n",
      "Uncle  :  uncl\n",
      "Vernon  :  vernon\n",
      ",  :  ,\n",
      "along  :  along\n",
      "son  :  son\n",
      "Dudley  :  dudley\n",
      "duration  :  durat\n",
      "series  :  seri\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "for w in cleaned_words:\n",
    "    print(w, \" : \", porter_stem.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c11473",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [porter_stem.stem(word) for word in cleaned_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8302bed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harri',\n",
       " 'special',\n",
       " 'young',\n",
       " 'wizard',\n",
       " '.',\n",
       " 'harri',\n",
       " \"'s\",\n",
       " 'parent',\n",
       " ',',\n",
       " 'jame',\n",
       " 'lili',\n",
       " 'potter',\n",
       " ',',\n",
       " 'kill',\n",
       " 'lord',\n",
       " 'voldemort',\n",
       " 'babi',\n",
       " '.',\n",
       " 'he',\n",
       " 'live',\n",
       " 'rais',\n",
       " 'aunt',\n",
       " 'petunia',\n",
       " ',',\n",
       " 'mother',\n",
       " \"'s\",\n",
       " 'sister',\n",
       " ',',\n",
       " 'uncl',\n",
       " 'vernon',\n",
       " ',',\n",
       " 'along',\n",
       " 'son',\n",
       " 'dudley',\n",
       " 'durat',\n",
       " 'seri',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e60f56",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839ae857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sneha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3db9440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdecbf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry\n",
      "life\n",
      "killed\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"Harry\"))\n",
    "print(lemmatizer.lemmatize(\"lives\"))\n",
    "print(lemmatizer.lemmatize(\"killed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56039d2",
   "metadata": {},
   "source": [
    "Part 2: Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e59b12",
   "metadata": {},
   "source": [
    "**Term Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951d083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16d69e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf values:\n",
      "along : 1\n",
      "and : 3\n",
      "aunt : 1\n",
      "baby : 1\n",
      "by : 2\n",
      "dudley : 1\n",
      "duration : 1\n",
      "for : 1\n",
      "harry : 2\n",
      "he : 2\n",
      "his : 2\n",
      "is : 2\n",
      "james : 1\n",
      "killed : 1\n",
      "lily : 1\n",
      "lives : 1\n",
      "lord : 1\n",
      "mother : 1\n",
      "of : 1\n",
      "parents : 1\n",
      "petunia : 1\n",
      "potter : 1\n",
      "raised : 1\n",
      "series : 1\n",
      "sister : 1\n",
      "son : 1\n",
      "special : 1\n",
      "the : 2\n",
      "their : 1\n",
      "uncle : 1\n",
      "vernon : 1\n",
      "voldemort : 1\n",
      "was : 1\n",
      "were : 1\n",
      "when : 1\n",
      "with : 2\n",
      "wizard : 1\n",
      "young : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "transformed_data = vectorizer.fit_transform(word_token)\n",
    "\n",
    "print('tf values:')\n",
    "for ele1, ele2 in zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0))):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acc35401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf values:\n",
      "along : 1\n",
      "aunt : 1\n",
      "babi : 1\n",
      "dudley : 1\n",
      "durat : 1\n",
      "harri : 2\n",
      "he : 1\n",
      "jame : 1\n",
      "kill : 1\n",
      "lili : 1\n",
      "live : 1\n",
      "lord : 1\n",
      "mother : 1\n",
      "parent : 1\n",
      "petunia : 1\n",
      "potter : 1\n",
      "rais : 1\n",
      "seri : 1\n",
      "sister : 1\n",
      "son : 1\n",
      "special : 1\n",
      "uncl : 1\n",
      "vernon : 1\n",
      "voldemort : 1\n",
      "wizard : 1\n",
      "young : 1\n"
     ]
    }
   ],
   "source": [
    "transformed_data = vectorizer.fit_transform(tokens)\n",
    "\n",
    "print('tf values:')\n",
    "for ele1, ele2 in zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0))):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d64a21",
   "metadata": {},
   "source": [
    "**Inverse Document Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a03b7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1a0371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c25eeccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf values:\n",
      "along : 4.401197381662156\n",
      "and : 3.70805020110221\n",
      "aunt : 4.401197381662156\n",
      "baby : 4.401197381662156\n",
      "by : 3.995732273553991\n",
      "dudley : 4.401197381662156\n",
      "duration : 4.401197381662156\n",
      "for : 4.401197381662156\n",
      "harry : 3.995732273553991\n",
      "he : 3.995732273553991\n",
      "his : 3.995732273553991\n",
      "is : 3.995732273553991\n",
      "james : 4.401197381662156\n",
      "killed : 4.401197381662156\n",
      "lily : 4.401197381662156\n",
      "lives : 4.401197381662156\n",
      "lord : 4.401197381662156\n",
      "mother : 4.401197381662156\n",
      "of : 4.401197381662156\n",
      "parents : 4.401197381662156\n",
      "petunia : 4.401197381662156\n",
      "potter : 4.401197381662156\n",
      "raised : 4.401197381662156\n",
      "series : 4.401197381662156\n",
      "sister : 4.401197381662156\n",
      "son : 4.401197381662156\n",
      "special : 4.401197381662156\n",
      "the : 3.995732273553991\n",
      "their : 4.401197381662156\n",
      "uncle : 4.401197381662156\n",
      "vernon : 4.401197381662156\n",
      "voldemort : 4.401197381662156\n",
      "was : 4.401197381662156\n",
      "were : 4.401197381662156\n",
      "when : 4.401197381662156\n",
      "with : 3.995732273553991\n",
      "wizard : 4.401197381662156\n",
      "young : 4.401197381662156\n"
     ]
    }
   ],
   "source": [
    "result = tfidf.fit_transform(word_token)\n",
    "print('idf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22a69a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf values:\n",
      "along : 3.9444389791664403\n",
      "aunt : 3.9444389791664403\n",
      "babi : 3.9444389791664403\n",
      "dudley : 3.9444389791664403\n",
      "durat : 3.9444389791664403\n",
      "harri : 3.538973871058276\n",
      "he : 3.9444389791664403\n",
      "jame : 3.9444389791664403\n",
      "kill : 3.9444389791664403\n",
      "lili : 3.9444389791664403\n",
      "live : 3.9444389791664403\n",
      "lord : 3.9444389791664403\n",
      "mother : 3.9444389791664403\n",
      "parent : 3.9444389791664403\n",
      "petunia : 3.9444389791664403\n",
      "potter : 3.9444389791664403\n",
      "rais : 3.9444389791664403\n",
      "seri : 3.9444389791664403\n",
      "sister : 3.9444389791664403\n",
      "son : 3.9444389791664403\n",
      "special : 3.9444389791664403\n",
      "uncl : 3.9444389791664403\n",
      "vernon : 3.9444389791664403\n",
      "voldemort : 3.9444389791664403\n",
      "wizard : 3.9444389791664403\n",
      "young : 3.9444389791664403\n"
     ]
    }
   ],
   "source": [
    "result = tfidf.fit_transform(tokens)\n",
    "print('idf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f66991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51563a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0621f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
